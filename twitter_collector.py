#!/usr/bin/env python3
"""
Twitter æ•°æ®é‡‡é›†å·¥å…·
ä½¿ç”¨ twscrape åº“ä» Twitter/X é‡‡é›†ç”¨æˆ·æ¨æ–‡

ä½¿ç”¨æ–¹æ³•ï¼š
1. å®‰è£…ä¾èµ–: pip install twscrape
2. æ·»åŠ è´¦å·: twscrape add_accounts
3. è¿è¡Œè„šæœ¬: python twitter_collector.py @username
"""

import asyncio
import json
import argparse
from datetime import datetime
from pathlib import Path
from twscrape import API, gather
from twscrape.logger import set_log_level


class TwitterCollector:
    """Twitter æ•°æ®é‡‡é›†å™¨"""

    def __init__(self, log_level="INFO"):
        """åˆå§‹åŒ–é‡‡é›†å™¨"""
        self.api = API()
        set_log_level(log_level)

    async def login(self):
        """ç™»å½•è´¦å·æ± """
        print("ğŸ” æ­£åœ¨ç™»å½•è´¦å·æ± ...")
        await self.api.pool.login()
        print("âœ“ è´¦å·æ± å·²ç™»å½•")

    async def get_user_info(self, username):
        """è·å–ç”¨æˆ·ä¿¡æ¯"""
        print(f"ğŸ” æ­£åœ¨æŸ¥æ‰¾ç”¨æˆ· @{username}...")
        try:
            user = await self.api.user_by_login(username)
            print(f"âœ“ æ‰¾åˆ°ç”¨æˆ·: {user.display_name} (@{username})")
            print(f"  - ID: {user.id}")
            print(f"  - ç²‰ä¸: {user.followersCount:,}")
            print(f"  - å…³æ³¨: {user.friendsCount:,}")
            print(f"  - æ¨æ–‡: {user.statusesCount:,}")
            return user
        except Exception as e:
            print(f"âœ— é”™è¯¯: {e}")
            return None

    async def get_user_tweets(self, username, limit=100):
        """
        è·å–æŒ‡å®šç”¨æˆ·çš„æ¨æ–‡

        Args:
            username: Twitter ç”¨æˆ·åï¼ˆä¸å« @ï¼‰
            limit: è·å–æ¨æ–‡æ•°é‡

        Returns:
            æ¨æ–‡åˆ—è¡¨
        """
        user = await self.get_user_info(username)
        if not user:
            return []

        print(f"\nğŸ“¥ æ­£åœ¨è·å–æ¨æ–‡ï¼ˆæœ€å¤š {limit} æ¡ï¼‰...")
        tweets = []

        try:
            async for tweet in self.api.user_tweets(user.id, limit=limit):
                tweets.append({
                    'id': str(tweet.id),
                    'date': tweet.date.isoformat(),
                    'content': tweet.rawContent,
                    'likes': tweet.likeCount,
                    'retweets': tweet.retweetCount,
                    'replies': tweet.replyCount,
                    'views': tweet.viewCount,
                    'url': f"https://twitter.com/{username}/status/{tweet.id}",
                    'author': username,
                    'author_id': str(user.id),
                })

                # å®æ—¶æ˜¾ç¤ºè¿›åº¦
                if len(tweets) % 10 == 0:
                    print(f"  å·²è·å– {len(tweets)} æ¡...")

            print(f"âœ“ æˆåŠŸè·å– {len(tweets)} æ¡æ¨æ–‡")
            return tweets

        except Exception as e:
            print(f"âœ— é‡‡é›†å¤±è´¥: {e}")
            return []

    async def get_following_list(self, username, limit=200):
        """
        è·å–ç”¨æˆ·çš„å…³æ³¨åˆ—è¡¨

        Args:
            username: Twitter ç”¨æˆ·å
            limit: æœ€å¤§å…³æ³¨æ•°é‡

        Returns:
            å…³æ³¨çš„ç”¨æˆ·åˆ—è¡¨
        """
        user = await self.get_user_info(username)
        if not user:
            return []

        print(f"\nğŸ‘¥ æ­£åœ¨è·å–å…³æ³¨åˆ—è¡¨...")
        following = []

        try:
            async for f in self.api.following(user.id, limit=limit):
                following.append({
                    'username': f.username,
                    'id': str(f.id),
                    'display_name': f.display_name,
                    'followers': f.followersCount,
                    'following': f.friendsCount,
                })

            print(f"âœ“ æ‰¾åˆ° {len(following)} ä¸ªå…³æ³¨çš„ç”¨æˆ·")
            return following

        except Exception as e:
            print(f"âœ— è·å–å¤±è´¥: {e}")
            return []

    async def get_following_tweets(self, username, tweets_per_user=20, max_users=10):
        """
        è·å–å…³æ³¨ç”¨æˆ·çš„æ‰€æœ‰æ¨æ–‡

        Args:
            username: Twitter ç”¨æˆ·å
            tweets_per_user: æ¯ä¸ªç”¨æˆ·è·å–çš„æ¨æ–‡æ•°
            max_users: æœ€å¤šè·å–å¤šå°‘ä¸ªç”¨æˆ·çš„æ¨æ–‡

        Returns:
            æ‰€æœ‰æ¨æ–‡åˆ—è¡¨
        """
        following = await self.get_following_list(username, limit=max_users)
        if not following:
            return []

        print(f"\nğŸ“¥ æ­£åœ¨ä¸‹è½½å…³æ³¨ç”¨æˆ·çš„æ¨æ–‡ï¼ˆæ¯äººæœ€å¤š {tweets_per_user} æ¡ï¼‰...")

        all_tweets = []
        for i, user in enumerate(following, 1):
            print(f"\n[{i}/{len(following)}] @{user['username']} ({user.get('display_name', 'N/A')})")
            tweets = await self.get_user_tweets(user['username'], limit=tweets_per_user)
            all_tweets.extend(tweets)

            # æ·»åŠ å»¶è¿Ÿé¿å…è¢«é™åˆ¶
            await asyncio.sleep(1)

        return all_tweets

    def save_json(self, data, filename):
        """ä¿å­˜ä¸º JSON æ ¼å¼"""
        output_dir = Path("twitter_data")
        output_dir.mkdir(exist_ok=True)

        filepath = output_dir / f"{filename}.json"
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"âœ“ å·²ä¿å­˜åˆ° {filepath}")
        return filepath

    def save_markdown(self, tweets, filename):
        """ä¿å­˜ä¸º Markdown æ ¼å¼ï¼ˆæ›´å¥½çš„å¯è¯»æ€§ï¼‰"""
        output_dir = Path("twitter_data")
        output_dir.mkdir(exist_ok=True)

        filepath = output_dir / f"{filename}.md"
        with open(filepath, 'w', encoding='utf-8') as f:
            # å†™å…¥æ ‡é¢˜
            f.write(f"# Twitter æ•°æ®é‡‡é›†\n\n")
            f.write(f"**é‡‡é›†æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**æ¨æ–‡æ•°é‡**: {len(tweets)}\n\n")
            f.write("---\n\n")

            # å†™å…¥æ¯æ¡æ¨æ–‡
            for tweet in tweets:
                f.write(f"## {tweet['date']}\n\n")
                f.write(f"**ä½œè€…**: @{tweet.get('author', 'N/A')}\n\n")
                f.write(f"{tweet['content']}\n\n")

                # äº’åŠ¨æ•°æ®
                f.write(f"**äº’åŠ¨æ•°æ®**:\n")
                f.write(f"- ğŸ‘ èµ: {tweet['likes']:,}\n")
                f.write(f"- ğŸ”„ è½¬: {tweet['retweets']:,}\n")
                f.write(f"- ğŸ’¬ å›: {tweet['replies']:,}\n")
                if tweet.get('views'):
                    f.write(f"- ğŸ‘ï¸ çœ‹: {tweet['views']:,}\n")

                f.write(f"\n**é“¾æ¥**: {tweet['url']}\n\n")
                f.write("---\n\n")

        print(f"âœ“ å·²ä¿å­˜åˆ° {filepath}")
        return filepath

    def save_csv(self, tweets, filename):
        """ä¿å­˜ä¸º CSV æ ¼å¼ï¼ˆä¾¿äº Excel æ‰“å¼€ï¼‰"""
        import csv

        output_dir = Path("twitter_data")
        output_dir.mkdir(exist_ok=True)

        filepath = output_dir / f"{filename}.csv"
        with open(filepath, 'w', encoding='utf-8-sig', newline='') as f:
            if not tweets:
                return

            writer = csv.DictWriter(f, fieldnames=[
                'date', 'author', 'content', 'likes', 'retweets',
                'replies', 'views', 'url'
            ])
            writer.writeheader()

            for tweet in tweets:
                writer.writerow({
                    'date': tweet['date'],
                    'author': tweet.get('author', ''),
                    'content': tweet['content'],
                    'likes': tweet['likes'],
                    'retweets': tweet['retweets'],
                    'replies': tweet['replies'],
                    'views': tweet.get('views', ''),
                    'url': tweet['url']
                })

        print(f"âœ“ å·²ä¿å­˜åˆ° {filepath}")
        return filepath


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='Twitter æ•°æ®é‡‡é›†å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # é‡‡é›†å•ä¸ªç”¨æˆ·çš„æ¨æ–‡
  python twitter_collector.py @elonmusk -n 100

  # é‡‡é›†å…³æ³¨ç”¨æˆ·çš„æ¨æ–‡
  python twitter_collector.py @myusername --following --max-users 20

  # ä»…ä¿å­˜ä¸º CSV
  python twitter_collector.py @username -n 50 --format csv
        """
    )

    parser.add_argument('username', help='Twitter ç”¨æˆ·åï¼ˆå¯ä»¥å¸¦ @ï¼‰')
    parser.add_argument('-n', '--number', type=int, default=100,
                       help='è·å–æ¨æ–‡æ•°é‡ï¼ˆé»˜è®¤: 100ï¼‰')
    parser.add_argument('--following', action='store_true',
                       help='è·å–è¯¥ç”¨æˆ·å…³æ³¨çš„æ‰€æœ‰äººçš„æ¨æ–‡')
    parser.add_argument('--max-users', type=int, default=10,
                       help='æœ€å¤šè·å–å¤šå°‘ä¸ªå…³æ³¨ç”¨æˆ·çš„æ¨æ–‡ï¼ˆé»˜è®¤: 10ï¼‰')
    parser.add_argument('--tweets-per-user', type=int, default=20,
                       help='æ¯ä¸ªç”¨æˆ·è·å–çš„æ¨æ–‡æ•°ï¼ˆé»˜è®¤: 20ï¼‰')
    parser.add_argument('--format', choices=['json', 'markdown', 'csv', 'all'],
                       default='all', help='è¾“å‡ºæ ¼å¼ï¼ˆé»˜è®¤: allï¼‰')
    parser.add_argument('--output', help='è¾“å‡ºæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰')
    parser.add_argument('--debug', action='store_true',
                       help='æ˜¾ç¤ºè°ƒè¯•æ—¥å¿—')

    args = parser.parse_args()

    # å»æ‰ @ ç¬¦å·
    username = args.username.lstrip('@')

    # è®¾ç½®è¾“å‡ºæ–‡ä»¶å
    output_name = args.output or f"{username}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

    # åˆ›å»ºé‡‡é›†å™¨
    collector = TwitterCollector(log_level="DEBUG" if args.debug else "INFO")

    # ç™»å½•
    await collector.login()

    # é‡‡é›†æ•°æ®
    if args.following:
        print(f"\n{'='*60}")
        print(f"é‡‡é›†æ¨¡å¼: å…³æ³¨åˆ—è¡¨")
        print(f"ç›®æ ‡ç”¨æˆ·: @{username}")
        print(f"æœ€å¤šç”¨æˆ·: {args.max_users}")
        print(f"æ¯ç”¨æˆ·æ¨æ–‡: {args.tweets_per_user}")
        print(f"{'='*60}\n")

        tweets = await collector.get_following_tweets(
            username,
            tweets_per_user=args.tweets_per_user,
            max_users=args.max_users
        )
    else:
        print(f"\n{'='*60}")
        print(f"é‡‡é›†æ¨¡å¼: å•ä¸ªç”¨æˆ·")
        print(f"ç›®æ ‡ç”¨æˆ·: @{username}")
        print(f"æ¨æ–‡æ•°é‡: {args.number}")
        print(f"{'='*60}\n")

        tweets = await collector.get_user_tweets(username, limit=args.number)

    if not tweets:
        print("\nâŒ æ²¡æœ‰è·å–åˆ°æ¨æ–‡")
        return

    # ä¿å­˜æ•°æ®
    print(f"\n{'='*60}")
    print("ğŸ’¾ æ­£åœ¨ä¿å­˜æ•°æ®...")
    print(f"{'='*60}\n")

    if args.format in ['json', 'all']:
        collector.save_json(tweets, output_name)

    if args.format in ['markdown', 'all']:
        collector.save_markdown(tweets, output_name)

    if args.format in ['csv', 'all']:
        collector.save_csv(tweets, output_name)

    print(f"\n{'='*60}")
    print("âœ… é‡‡é›†å®Œæˆï¼")
    print(f"{'='*60}")
    print(f"\næ•°æ®ç»Ÿè®¡:")
    print(f"  æ€»æ¨æ–‡æ•°: {len(tweets)}")
    print(f"  æ€»ç‚¹èµæ•°: {sum(t['likes'] for t in tweets):,}")
    print(f"  æ€»è½¬å‘æ•°: {sum(t['retweets'] for t in tweets):,}")
    print(f"  æ€»å›å¤æ•°: {sum(t['replies'] for t in tweets):,}")

    if tweets:
        print(f"\næ—¶é—´èŒƒå›´:")
        dates = [t['date'] for t in tweets]
        print(f"  æœ€æ—©: {min(dates)}")
        print(f"  æœ€æ–°: {max(dates)}")

    print(f"\næ–‡ä»¶ä¿å­˜åœ¨: twitter_data/")

    # æ˜¾ç¤ºä½¿ç”¨æç¤º
    print(f"\nğŸ’¡ æç¤º:")
    print(f"  - JSON æ ¼å¼é€‚åˆç¨‹åºå¤„ç†")
    print(f"  - Markdown æ ¼å¼é€‚åˆäººç±»é˜…è¯»")
    print(f"  - CSV æ ¼å¼å¯ç”¨ Excel æ‰“å¼€")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
